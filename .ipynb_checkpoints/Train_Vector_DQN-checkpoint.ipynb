{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Agents: Banana Collector \n",
    "Double DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/Unity-Technologies/ml-agents.git\n",
    "# !git -C ml-agents checkout 0.4.0b\n",
    "\n",
    "# !sudo pip3 install ml-agents/python/.\n",
    "\n",
    "# !pip install grpcio\n",
    "# !pip install seaborn\n",
    "# !pip install tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux_NoVis.zip\n",
    "# !unzip Banana_Linux_NoVis.zip\n",
    "# !mv Banana_Linux_NoVis/ data/\n",
    "# !rm Banana_Linux_NoVis.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from dqn_agent import Agent\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from IPython.display import FileLink, FileLinks\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"/home/vikhanna/Banana_Linux_NoVis/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "state = env_info.vector_observations[0]\n",
    "state_size = len(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_env(env, brain_name):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    return env_info.vector_observations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_env(env, action, brain_name):\n",
    "    env_info = env.step(action)[brain_name]\n",
    "    next_state = env_info.vector_observations[0]\n",
    "    reward = env_info.rewards[0] \n",
    "    done = env_info.local_done[0]  \n",
    "    return (next_state, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_replay_memory(agent, env, action_size, brain_name, size=50000):\n",
    "    state = reset_env(env, brain_name)\n",
    "    it = 0\n",
    "    while it < size:\n",
    "        action = np.random.randint(action_size)\n",
    "        next_state, reward, done = step_env(env,action,brain_name)\n",
    "        agent.step(state, action, reward, next_state, done, train=False)\n",
    "        \n",
    "        it+=1 \n",
    "        if done:\n",
    "            state = reset_env(env, brain_name)\n",
    "        else:\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, scores, env, brain_name, episodes, steps, replay_start_size, \n",
    "          inital_eps, final_eps, final_exp_ep,\n",
    "          print_every=10, out_file='model_v1.ckpt'):\n",
    "    #init_replay_memory(agent, env, replay_start_size)\n",
    "    \n",
    "    eps = initial_eps\n",
    "    last_saved = 0 \n",
    "    it = 0\n",
    "\n",
    "    with tnrange(episodes) as t: \n",
    "        for ep_i in t: \n",
    "            state = reset_env(env, brain_name)\n",
    "            score = 0\n",
    "            for step_i in range(steps):\n",
    "                action = agent.act(state, epsilon=eps)\n",
    "                next_state, reward, done = step_env(env,action,brain_name)\n",
    "                score += reward\n",
    "                agent.step(state, action, reward, next_state, done)\n",
    "                it += 1 \n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "            eps = max(final_eps,initial_eps-(initial_eps-final_eps)/final_exp_ep*ep_i) \n",
    "            scores.add(score)\n",
    "        \n",
    "            if (ep_i+1) % print_every == 0:\n",
    "                t.set_postfix(epsilon=eps, score=scores.last)\n",
    "        \n",
    "            if scores.last >= 13.0 and scores.last > last_saved:\n",
    "                last_saved = scores.last\n",
    "                print('Saving...')\n",
    "                agent.save(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_search(scores, env, brain_name, \n",
    "                 lrates, \n",
    "                 final_exp_eps, \n",
    "                 taus, \n",
    "                 final_epsilons, \n",
    "                 gammas,\n",
    "                 episodes=500, \n",
    "                 iterations=10):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    action_size = brain.vector_action_space_size\n",
    "    state = env_info.vector_observations[0]\n",
    "    state_size = len(state)\n",
    "    \n",
    "    for it in range(iterations):\n",
    "        lrate = lrates[np.random.randint(len(lrates))]\n",
    "        final_exp_ep = final_exp_eps[np.random.randint(len(final_exp_eps))]\n",
    "        tau = taus[np.random.randint(len(taus))] \n",
    "        final_eps = final_epsilons[np.random.randint(len(final_epsilons))] \n",
    "        gamma = gammas[np.random.randint(len(gammas))] \n",
    "        \n",
    "        agent = Agent(\n",
    "            state_size, \n",
    "            action_size, \n",
    "            seed=0, \n",
    "            replay_size=replay_size,\n",
    "            tau = tau,\n",
    "            lrate = lrate, \n",
    "            gamma = gamma\n",
    "        )\n",
    "        \n",
    "        score = helpers.MovingResult(100, params = agent.params, name='score')\n",
    "        score.params.add('final_exp_ep', final_exp_ep) \n",
    "        score.params.add('final_eps', final_eps)\n",
    "        scores.append(score)\n",
    "        \n",
    "        print(f'-- Iteration {it+1}/{iterations} --')\n",
    "        print(f'{score.params}')\n",
    "        \n",
    "        train(\n",
    "            agent, \n",
    "            score, \n",
    "            env, \n",
    "            brain_name,\n",
    "            episodes, \n",
    "            steps, \n",
    "            replay_start_size, \n",
    "            initial_eps, final_eps, \n",
    "            final_exp_ep\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores):\n",
    "    plt.figure(figsize=(10,5), dpi=80)\n",
    "    \n",
    "    for i in range(len(scores)):\n",
    "        print(f'Test #{i+1}: {scores[i].params}')\n",
    "        x_moving, y_moving = zip(*scores[i].buffer_moving)\n",
    "        plt.plot(list(x_moving), list(y_moving), label=f'Test #{i+1}')\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.ylabel(f'Moving Score (100)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(env, brain_name, agent):\n",
    "    scores = np.zeros(100)\n",
    "    for ep_i in range(scores.shape[0]):\n",
    "        state = reset_env(env, brain_name)\n",
    "        score = 0\n",
    "        while True:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = step_env(env,action,brain_name)\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            if done: \n",
    "                scores[ep_i] = score\n",
    "                break    \n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'replay_buffer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-41e8d2eeaba2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mreplay_buffer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmoving_result\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMovingResult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'replay_buffer'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import tensorflow as tf\n",
    "import gym \n",
    "import tflearn\n",
    "from replay_buffer import ReplayBuffer\n",
    "from moving_result import MovingResult\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, layers, seed=None):\n",
    "        super(QNetwork, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        ls = [nn.Linear(state_size, layers[0])]\n",
    "        ls.extend([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        ls.append(nn.Linear(layers[-1], action_size))\n",
    "        \n",
    "        self.layers = nn.ModuleList(ls)\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(l) for l in layers])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.layers[:-1]):\n",
    "            x = self.bns[i](F.relu(l(x)))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Double DQN Agent with epsilon-greedy policy \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    state_size: int \n",
    "        Size of the observation space\n",
    "    action_size: int\n",
    "        Number of discrete actions\n",
    "    seed: int \n",
    "        Number for random seeding\n",
    "    replay_size: int\n",
    "        Size of the experience replay buffer\n",
    "    batch_size: int\n",
    "        Size of the batch used when learning\n",
    "    gamma: float\n",
    "        Discount rate\n",
    "    lrate: int or float\n",
    "        Learning rate \n",
    "    tau: float\n",
    "        Soft target update rate\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size=None, action_size=None, seed=None,\n",
    "                 replay_size = 100000, batch_size=64, \n",
    "                 gamma=0.99, lrate=5e-4, tau=0.001, update_every = 4, \n",
    "                 hidden_layers = [250, 500, 250],\n",
    "                 restore=None):\n",
    "\n",
    "        if restore is not None:\n",
    "            checkpoint = torch.load(restore, map_location={'cuda:0': 'cpu'})\n",
    "\n",
    "        self.seed = seed if not restore else checkpoint['seed']\n",
    "        self.action_size = action_size if not restore else checkpoint['action_size']\n",
    "        self.state_size = state_size if not restore else checkpoint['state_size']\n",
    "        self.replay_size = replay_size if not restore else checkpoint['replay_size']\n",
    "        self.batch_size = batch_size if not restore else checkpoint['batch_size']\n",
    "        self.gamma = gamma if not restore else checkpoint['gamma']\n",
    "        self.lrate = lrate if not restore else checkpoint['lrate']\n",
    "        self.tau = tau if not restore else checkpoint['tau']\n",
    "        self.update_every = update_every if not restore else checkpoint['update_every']\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.hidden_layers = hidden_layers if not restore else checkpoint['hidden_layers']\n",
    "        \n",
    "        self.q_network = QNetwork(\n",
    "            self.state_size, \n",
    "            self.action_size, \n",
    "            self.hidden_layers, \n",
    "            self.seed\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.q_network_target = QNetwork(\n",
    "            self.state_size, \n",
    "            self.action_size, \n",
    "            self.hidden_layers, \n",
    "            self.seed\n",
    "        ).to(self.device)\n",
    "        \n",
    "        if restore is not None:\n",
    "            self.q_network.load_state_dict(checkpoint['state_dict'])\n",
    "            self.q_network_target.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.lrate)\n",
    "        \n",
    "#         self.reset()\n",
    "        \n",
    "    def save(self, filename):\n",
    "        checkpoint = self.params\n",
    "        checkpoint['state_dict'] = self.q_network.state_dict()\n",
    "        torch.save(checkpoint, filename)\n",
    "        \n",
    "#     def reset(self):\n",
    "#         self.it = 0\n",
    "#         self.memory = ReplayBuffer(\n",
    "#             self.action_size, self.replay_size, \n",
    "#             self.update_every * self.batch_size, \n",
    "#             self.seed, self.device\n",
    "#         )\n",
    "#         self.losses = MovingResult(100, name='loss', save_raw=False)\n",
    "\n",
    "\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done, train=True):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.it += 1 \n",
    "        if train and self.it % self.update_every and len(self.memory) > self.memory.batch_size:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "    \n",
    "    def act(self, state, epsilon=0.):\n",
    "        \"\"\" Epsilon-Greedy policy\n",
    "        \n",
    "        \"\"\"\n",
    "        probs = epsilon * np.ones(self.action_size) / self.action_size\n",
    "        state = torch.from_numpy(state).float().to(self.device)\n",
    "        \n",
    "        self.q_network.eval()\n",
    "        with torch.no_grad():\n",
    "            probs[np.argmax(self.q_network(state.reshape(1,-1)).cpu().numpy())] += 1-epsilon\n",
    "        self.q_network.train()\n",
    "        \n",
    "        return np.random.choice(np.arange(self.action_size), p=probs)\n",
    "    \n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        # argmax_a Q^(next_state,a, w)\n",
    "        with torch.no_grad():\n",
    "            _ , best_actions = self.q_network(next_states).max(dim=1)\n",
    "            # y^ = td_target\n",
    "            # y^ = reward + gamma * Q^(next_state,argmax_a(next_state,a, w), w-), episode not terminal\n",
    "            # y^ = reward, episode terminal\n",
    "            td_targets = rewards + self.gamma * torch.gather(self.q_network_target(next_states),1,best_actions.view(-1,1))\n",
    "            for i in range(self.memory.batch_size):\n",
    "                if dones[i].item() == 1.0:\n",
    "                    td_targets[i] = rewards[i]   \n",
    "        \n",
    "        # delta = y^-Q\n",
    "        # clamp btwn -1..1\n",
    "        delta = torch.clamp(td_targets-torch.gather(self.q_network(states),1,actions), -1., 1.)\n",
    "        # loss = sum (y^-Q)^2\n",
    "        loss = torch.sum(torch.pow(delta,2))\n",
    "        self.losses.add(loss.item()/self.memory.batch_size)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.soft_update()\n",
    "            \n",
    "    def soft_update(self):\n",
    "        \"\"\"Soft update of target network\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        \"\"\"\n",
    "        for target_param, param in zip(self.q_network_target.parameters(), self.q_network.parameters()):\n",
    "            target_param.data.copy_(self.tau*param.data+(1-self.tau)*target_param.data)\n",
    "            \n",
    "    @property\n",
    "    def params(self):\n",
    "        params = {\n",
    "            'state_size': self.state_size,\n",
    "            'action_size': self.action_size,\n",
    "            'seed': self.seed,\n",
    "            'replay_size': self.replay_size,\n",
    "            'batch_size': self.batch_size,\n",
    "            'gamma': self.gamma, \n",
    "            'lrate': self.lrate,\n",
    "            'tau': self.tau,\n",
    "            'update_every': self.update_every,\n",
    "            'hidden_layers': self.hidden_layers\n",
    "        }\n",
    "        return params\n",
    "    \n",
    "    @property \n",
    "    def trainable_params(self):\n",
    "        return sum([np.prod(p.size()) for p in self.q_network.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_exp_ep = 500\n",
    "\n",
    "# 1.0 -> 0.1\n",
    "initial_eps = 1.0\n",
    "final_eps = 0.01 \n",
    "\n",
    "episodes = 1000\n",
    "steps = 2000\n",
    "replay_start_size = 5000\n",
    "replay_size = 100000\n",
    "tau = 0.05 \n",
    "lrate = 5.0e-5\n",
    "update_every = 4\n",
    "batch_size = 32\n",
    "hidden_layers = [100, 200, 100]\n",
    "\n",
    "print_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'replay_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-083864ca1c26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mupdate_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_every\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mhidden_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'replay_size'"
     ]
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    state_size, \n",
    "    action_size, \n",
    "    seed=0, \n",
    "    replay_size=replay_size,\n",
    "    tau = tau,\n",
    "    lrate = lrate,\n",
    "    batch_size = batch_size,\n",
    "    update_every = update_every,\n",
    "    hidden_layers = hidden_layers\n",
    ")\n",
    "\n",
    "scores = helpers.MovingResult(100, name='score', save_raw=False)\n",
    "\n",
    "scores_agent = train(\n",
    "    agent, \n",
    "    scores, \n",
    "    env, \n",
    "    brain_name,\n",
    "    episodes, \n",
    "    steps, \n",
    "    replay_start_size, \n",
    "    initial_eps, final_eps, \n",
    "    final_exp_ep,\n",
    "    out_file='model_v1.ckpt',\n",
    "    print_every = print_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Path (<tt>model_v1.ckpt</tt>) doesn't exist. It may still be in the process of being generated, or you may have the incorrect path."
      ],
      "text/plain": [
       "/home/vikhanna/udacity/RLnanodegree/submission/model_v1.ckpt"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink('model_v1.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.plot()\n",
    "agent.losses.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "hyper_search(\n",
    "    scores,\n",
    "    env, brain_name, \n",
    "    final_exp_eps = [250],\n",
    "    taus=[0.05],\n",
    "    lrates = [0.1e-4, 0.5e-4, 1.0e-4],\n",
    "    final_epsilons = [0.1, 0.01],\n",
    "    gammas = [0.9, 0.95, 0.99],\n",
    "    iterations = 25\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
